{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util - Generate Training Dataset\n",
    "* StelllarAlgo Data Science\n",
    "* Ryan Kazmerik & Nakisa Rad\n",
    "* May 7, 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import pandas as pd\n",
    "import pyodbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a connection to MSSQL PROD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to SQL Server.\n",
    "SERVER = '34.206.73.189' \n",
    "DATABASE = 'datascience' \n",
    "USERNAME = 'dsAdminWrite' \n",
    "PASSWORD = getpass.getpass(prompt='Enter your password')\n",
    "CNXN = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+SERVER+';DATABASE='+DATABASE+';UID='+USERNAME+';PWD='+ PASSWORD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create a list of databases we would like to use to generate the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEAMS = [\n",
    "    {\n",
    "            \"mssql_dbname\": \"stlrMLS\",\n",
    "            \"lkupclientid\": \"31\",\n",
    "            \"clientcode\": \"sacfc\",\n",
    "            \"train_year\": 2021,\n",
    "            \"test_year\": 2022\n",
    "        },\n",
    "        {   \n",
    "            \"mssql_dbname\": \"stlrUSLLocomotive\",\n",
    "            \"lkupclientid\": \"99\",\n",
    "            \"clientcode\": \"usllocomotive\",\n",
    "            \"train_year\": 2021,\n",
    "            \"test_year\": 2022\n",
    "        }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can run our stored proc to get retention training and inference datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GETTING TEAM DATASETS:\n",
      " > ADDING TEAM TO DATASET: sacfc\n",
      " > ADDING TEAM TO DATASET: usllocomotive\n",
      "TOTAL TEAMS IN DATASET: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"GETTING TEAM DATASETS:\")\n",
    "\n",
    "team_datasets = []\n",
    "for team in TEAMS:\n",
    "\n",
    "    cursor = CNXN.cursor()\n",
    "\n",
    "    storedProc = (\n",
    "        f\"\"\"Exec {team[\"mssql_dbname\"]}.[ds].[getRetentionScoringModelData] {team[\"lkupclientid\"]}\"\"\"\n",
    "    )\n",
    "\n",
    "    df = pd.read_sql(storedProc, CNXN)\n",
    "\n",
    "    df[\"year\"] = pd.to_numeric(df[\"year\"])\n",
    "    df = df[df[\"year\"] <= team[\"train_year\"]]\n",
    "\n",
    "    print(f\" > ADDING TEAM TO DATASET: {team['clientcode']}\")\n",
    "\n",
    "    CNXN.commit()\n",
    "    cursor.close()\n",
    "\n",
    "    team_datasets.append(df)\n",
    "\n",
    "print(f\"TOTAL TEAMS IN DATASET: {len(team_datasets)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create one dataframe for all team datasets and see how much data we have in total and for each year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12836, 54)\n",
      "2021    3544\n",
      "2019    2447\n",
      "2017    2440\n",
      "2018    2292\n",
      "2020    2096\n",
      "2016      17\n",
      "Name: year, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_dataset = pd.concat(team_datasets)\n",
    "\n",
    "print(df_dataset.shape)\n",
    "print(df_dataset.year.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's partition this dataframe into 4 equal sized dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH 1 SIZE: 3209\n",
      "BATCH 2 SIZE: 3209\n",
      "BATCH 3 SIZE: 3209\n",
      "BATCH 4 SIZE: 3209\n"
     ]
    }
   ],
   "source": [
    "batch_size = int(len(df_dataset) / 4) # create 4 equal batches\n",
    "df_batches = [df_dataset[i:i + batch_size] for i in range(0, df_dataset.shape[0], batch_size)]\n",
    "df_batches = df_batches[:4] # cut off straglers in last df\n",
    "\n",
    "print(f\"BATCH 1 SIZE: {len(df_batches[0])}\")\n",
    "print(f\"BATCH 2 SIZE: {len(df_batches[1])}\")\n",
    "print(f\"BATCH 3 SIZE: {len(df_batches[2])}\")\n",
    "print(f\"BATCH 4 SIZE: {len(df_batches[3])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can convert these dataframes to parquet files and write them to a directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, df_batch in enumerate(df_batches):\n",
    "\n",
    "    df_batch.to_parquet(f\"./exports/000{idx}_part_00.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "89f48dadab51a6b5b48ffc1a9e043c21cefe67453637c3444f3b86917b8aad16"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('stellar')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
